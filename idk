%matplotlib inline
import tensorflow as tf
from tensorflow import keras
import numpy as np
from tensorflow.contrib import rnn
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score, accuracy_score, recall_score, precision_score
import os
import matplotlib.pyplot as plt
import warnings
from text_unidecode import unidecode
from collections import deque
warnings.filterwarnings('ignore')

import pandas as pd
from sklearn.manifold import TSNE
import networkx as nx
import matplotlib.patches as mpatches
import seaborn as sns
from node2vec import Node2Vec

# load data (figure out how to generate a traffic using flows.py and the connectivity dataset

def listdir_nohidden(path):
    for f in os.listdir(path):
        if not f.startswith('.'):
            yield f

# load data

data_dir = './test-data'
rootX = os.path.join(data_dir, 'x')
pathsX = sorted(listdir_nohidden (rootX))

rootY = os.path.join(data_dir, 'y')
pathsY = sorted(listdir_nohidden(rootY))

rootAdj = os.path.join(data_dir, 'adj')
pathsAdj = sorted(listdir_nohidden(rootAdj))

loadX = False
loadY = False
loadAdj = False


for x in pathsX:
    if not loadX:
        X = np.loadtxt(os.path.join(rootX, x))
        loadX = True
    else:
        X = np.vstack([X, np.loadtxt(os.path.join(rootX, x))])

for y in pathsY:
    if not loadY:
        Y = np.loadtxt(os.path.join(rootY, y))
        loadY = True
    else:
        Y = np.vstack([Y, np.loadtxt(os.path.join(rootY, y))])

for adj in pathsAdj:
    if not loadAdj:
        Adj = np.loadtxt(os.path.join(rootAdj, adj))
        loadAdj = True
    else:
        Adj = np.vstack([Adj, np.loadtxt(os.path.join(rootAdj, adj))])

# normalize between [0, 1]

X /= np.amax(X)
Y /= np.amax(Y)

Adj_Y = np.concatenate(Adj, Y, axis = 1)

# generate a graph

for adj in Adj_Y:
    graph = nx.Graph()
    for i, edge in enumerate(adj[:16]):
        if edge==1.0:
            grah.add_edge(i//4, i%4, weight=adj[16+i])
        else:
            graph.add_edge(i//4, i%4, weight=0.0)

    node2vec = Node2Vec(graph, dimensions=20, walk_length=16, num_walks=100, workers=2, weight_key='weight')
    model = node2vec.fit(window=10, min_count=1)
# take features and labels from the data respectively
features = Y
labels = X
# features = Y
# labels = Adj

# plt.scatter(features, labels)
# plt.show()


# Per normal, we split the data into training set and test set, the ratio is 0.2
# we want to keep the time steps consistent as the original, so set shuffle=False
# random_state=42 guarantees that same sequence of random numbers are generated each time we run the code
X_train,X_test,y_train,y_test = train_test_split(features, labels, test_size=0.2, shuffle=False, random_state=42)


# set hyperparameters:
# lets have 5 iterations first
epochs = 50

# we are doing a binary classification here, so 1 is enough. (0 means no flow going on between two nodes, 1 means yes)
n_classes = 1

# the size of the hidden_states, we have a relatively small model(?), so set it to 256.
n_units = 256

# size of the feature-data (unknown yet)
# n*(n-1)
# n_features = 32
n_features = 16

# self-explanatory
batch_size = 1


# shape of place holder when training (<batch size>, feature-size)
xplaceholder= tf.placeholder('float', [None, n_features])

# shape of place holder when training (<batch size>,)
yplaceholder = tf.placeholder('float')


# Design the model
def recurrent_neural_network_model():

    # set up weight and biases variables
    # alternative: use rnn.layers.fully_connected() to set up automatically
    layer = {'w1': tf.Variable(tf.random_normal([n_units, n_classes])),
             'w2': tf.Variable(tf.random_normal([n_units, n_classes])),
             'w3': tf.Variable(tf.random_normal([n_units, n_classes])),
             'bias': tf.Variable(tf.random_normal([n_classes]))}


    # split the 2D feature batch Tensor into n_feature slices.
    # each slice is an element of the sequence given as input to the LSTM layer
    # shape: <batch_size, 1>
    x = tf.split(xplaceholder, n_features, 1)

    # print(x)


    # instantiates variables for all gates
    # lstm_cell = keras.layers.LSTMCell(n_units)

    # the accuracy of the LSTM can be improved by additional layers.
    lstm_cell = rnn.MultiRNNCell([rnn.BasicLSTMCell(n_units), rnn.BasicLSTMCell(n_units)])


    # outputs contain all the outputs of the LSTM layer for each time step
    # states contain the values of the last state of both the hidden states (h&c)
    # outputs = keras.layers.RNN(lstm_cell, x, dtype=tf.float32, unroll=True, return_state=True)
    # output = tf.convert_to_tensor(output)
    outputs, states = tf.nn.static_rnn(lstm_cell, x, dtype=tf.float32)


    # forward propagation: wx + b
    # output = tf.matmul(output, layer['weights']) + layer['bias']
    wx = tf.linalg.matmul(outputs[-1], layer['w1']) + layer['bias']
    w2x2 = tf.add(tf.linalg.matmul(tf.pow(outputs[-1], 2), layer['w2']), wx)
    w3x3 = tf.add(tf.linalg.matmul(tf.pow(outputs[-1], 3), layer['w3']), w2x2)


    # normalize the data

    return w3x3

# class MinimalRNNCell(keras.layers.Layer):
#
#     def __init__(self, units, **kwargs):
#         self.units = n_units
#         self.state_size = n_units
#         super(MinimalRNNCell, self).__init__(**kwargs)
#
#     def build(self, input_shape):
#         self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
#                                       initializer='uniform',
#                                       name='kernel')
#         self.recurrent_kernel = self.add_weight(
#             shape=(self.units, self.units),
#             initializer='uniform',
#             name='recurrent_kernel')
#         self.built = True
#
#     def call(self, inputs, states):
#         prev_output = states[0]
#         h = K.dot(inputs, self.kernel)
#         output = h + K.dot(prev_output, self.recurrent_kernel)
#         return output, [output]
#
# # Let's use this cell in a RNN layer:
#
# cell = MinimalRNNCell(32)
# x = keras.Input((None, 5))
# layer = RNN(cell)
# y = layer(x)
#
# # Here's how to use the cell to build a stacked RNN:
#
# cells = [MinimalRNNCell(32), MinimalRNNCell(64)]
# x = keras.Input((None, 5))
# layer = RNN(cells)
# y = layer(x)


# Training starts
def train_neural_network():
    logit = recurrent_neural_network_model()

    # reshape the logit so that it shares the same shape of the labels
    logit = tf.reshape(logit, [-1])

    # binary classification -> sigmoid
    # multiclass -> softmax
    # the cost measures how wrong we are; thus, we are trying to minimize it
    cost = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=logit, labels=yplaceholder))


    # AdamOptimizer has good performance on minimizing the cost
    # optimizer = tf.train.AdamOptimizer().minimize(cost)
    learning_rate = 0.01
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)

    # Here dive into a true TF session
    with tf.Session() as sess:

        # initialize all global&local variables so far
        tf.global_variables_initializer().run()
        tf.local_variables_initializer().run()

        # define a loop using the epochs variable we set
        for epoch in range(epochs):

            # in each epoch, the epoch_loss starts from 0
            epoch_loss = 0

            i = 0

            # define another loop which ends until i hits the threshold we set
            for i in range(int(len(X_train) / batch_size)):
                start = i
                end = i + batch_size

                # assign a batch of features and labels respectively to batch_x and batch_y
                batch_x = np.array(X_train[start:end])
                batch_y = np.array(y_train[start:end])

                # MAGIC: here we tell the TF to run the subgraph necessary to compute the optimizer and the cost
                # by feeding the values in 'batch_x' and 'batch_y' to placeholders
                # the value of the optimizer would be thrown away
                # cost would be kept as c

                _, c = sess.run([optimizer, cost], feed_dict={xplaceholder: batch_x, yplaceholder: batch_y})


                # aggregates c to epoch_loss
                epoch_loss += c

                # i += batch_size

            # print each epoch_loss in each epoch
            print('Epoch', epoch, 'completed out of', epochs, 'loss:', epoch_loss)

        # Test the model

        # feed the testing data into the model and tell TF to run the subgraph necessary to compute the logit
        # pass the logit value through a sigmoid activation to get the prediction
        # rounded off to remove the decimal places of the predicted values
        # pred = tf.round(tf.nn.sigmoid(logit)).eval({xplaceholder: np.array(X_test), yplaceholder: np.array(y_test)})

        # calculate f1 score: weighted average of precision and recall
        # f-measure combines the power to avoid the accuracy paradox
        # f1 = f1_score(np.array(y_test), pred, average='macro')

        # calculate accuracy
        # accuracy = accuracy_score(np.array(y_test), pred)
        # tf.metrics.accuracy(np.array(y_test), pred)

        # calculate recall: ratio of correctly predicted positive observations to all positive observations
        # out of all the positive examples there were, what fraction did the classifier pick up?
        # recall = recall_score(y_true=np.array(y_test), y_pred=pred)


        # calculate precision:
        # ratio of correctly predicted positive obervations to the total predicted positive observations
        # out of all the examples the classifier labeled as positive, what fraction were correct?
        # precision = precision_score(y_true=np.array(y_test), y_pred=pred)

        # print all scores out
        # print("F1 Score:", f1)
        # print("Accuracy Score:", accuracy)
        # print("Recall:", recall)
        # print("Precision:", precision)


train_neural_network()
